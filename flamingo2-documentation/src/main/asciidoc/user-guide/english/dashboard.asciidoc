[[dashboard]]

== 워크플로우 모니터링

워크플로우 모니터링은 워크플로우 디자이너에서 실행한 워크플로우의 상태를 모니터링할 때 사용합니다.

image::dashboard/dashboard1.png[scaledwidth=100%,워크플로우 모니터링]

워크플로우 모니터링에서는 다음의 정보를 확인할 수 있습니다.

* 워크플로우 요약 정보
* 워크플로우 실행 상태 및 처리 시간
* 워크플로우를 구성하는 각 단계별 상태 및 로그
* 각 단계별 스크립트 및 에러 로그
* 각 단계별 MapReduce Job ID, YARN 애플리케이션 ID
* 워크플로우의 실행 이력 조희

[NOTE]
각 단계별 실행시 MapReduce를 기반으로 동작하는 Pig, Hive, MapReduce의 경우 Flamingo MapReduce Job Agent가 정상적으로 설정되어 있어야만 MapReduce Job ID 및 YARN 애플리케이션 ID를 추출할 수 있습니다.

=== 워크플로우 실행 이력의 추이 그래프

워크플로우의 실행 이력에 대한 추이 그래프를 표시하는 화면으로 최근 1주일 동안 1시간 단위의 통계 정보를 기반으로 다음과 같은 추이 그래프를 생성합니다.

image::dashboard/dashboard2.png[scaledwidth=100%,워크플로우 실행 이력의 추이 그래프]

=== 워크플로우 실행 이력 조회

워크플로우 실행 이력을 표시하는 목록으로 최근 실행한 이력이 가장 위에 표시됩니다. 로그인 사용자에 맞춰서 표시가 되며 관리자는 모든 실행 이력을 조회할 수 있습니다.

image::dashboard/dashboard3.png[scaledwidth=100%,워크플로우 실행 이력 조회]

화면의 각 항목은 다음의 기능을 제공합니다.

[width="80%",cols="5,20",options="header"]
|=======
|기능  |설명
|시작일 |지정한 날짜이후에 실행된 워크플로우를 조회할때 지정하는 시작일입니다.
|종료일 |지정한 날짜이전까지 실행된 워크플로우를 조회할때 지정하는 종료일입니다.
|상태 |조회할 워크플로우의 실행 상태코드입니다.
|워크플로우명 |워크플로우명으로 조회할때 사용하는 조건입니다. like 패턴으로 조회합니다.
|조회 |입력한 조회조건에 해당하는 워크플로우의 실행 이력을 조회합니다.
|초기화 |입력한 조회조건을 초기화합니다. 전체를 다시 조회하고자 할 때 초기화 버튼을 선택하고 조회 버튼을 선택하면 전체 목록을 조회할 수 있습니다.
|갱신 |입력한 조회조건에 따라 실행 이력을 조회합니다.
|=======

=== 워크플로우 실행 이력 보기

다음과 같이 워크플로우의 실행 이력이 목록으로 표시가 되면 보고자 하는 항목을 더블클릭합니다. 그러면 다음과 같이 워크플로우 실행 이력이 표시됩니다. 이력의 상단에는 워크플로우를 구성하는 각 단계의 실행 상태와 처리 시간이 표시되며 해당 항목의 상태를 상세하게 보고자 한다면 마우스 클릭하도록 합니다. 마우스를 클릭하면 가장 먼저 첫번째 실행 로그 탭에서는 해당 단계가 실행했을 때 남긴 로그 메시지를 표시합니다.

image::dashboard/dashboard4.png[scaledwidth=70%,워크플로우 실행 이력 보기]

두번째 탭은 각 단계가 실행하는데 필요한 커맨드 라인 정보가 표시됩니다.

image::dashboard/dashboard5.png[scaledwidth=70%,워크플로우 실행 이력 보기]

세번째 탭은 스크립트 정보가 표시되며 Hive의 경우 Hive QL이 표시되고, Pig의 경우 Pig Latin Script가 표시됩니다. Bash, R, Python의 경우도 사용자가 입력한 스크립트가 표시됩니다.

image::dashboard/dashboard6.png[scaledwidth=70%,워크플로우 실행 이력 보기]

네번째 탭은 에러 메시지가 표시되며 실행시 에러 메시지가 발생한 경우 에러 메시지를 모아서 표시합니다.

image::dashboard/dashboard7.png[scaledwidth=70%,워크플로우 실행 이력 보기]

다섯번재 탭은 YARN 및 MapReduce Job의 ID를 표시합니다.

image::dashboard/dashboard8.png[scaledwidth=70%,워크플로우 실행 이력 보기]

각 단계별 상태를 확인할 수 있지만 다음과 같이 워크플로우 전체 상태를 확인할 수 도 있습니다.

image::dashboard/dashboard9.png[scaledwidth=70%,워크플로우 실행 이력 보기]

=== YARN 애플리케이션 ID 및 MapReduce Job ID 모니터링

MapReduce Job의 경우 Flamingo MapReduce Job Agent를 통해서 YARN 애플리케이션 및 MapReduce Job ID를 수집합니다.
YARN 애플리케이션 및 MapReduce Job ID를 클라이언트측에서 수집함으로써
MapReduce Job을 추적할 수 있게 되며 Flamingo에서는 이 정보를 활용하여 워크플로우와 MapReduce Job의 연관 정보를 찾아냅니다.
MapReduce Job의 Job ID를 추출하는 Flamingo MapReduce Job Agent는 다음을 대상으로 동작합니다.

* MapReduce Job
* Pig Latin
* Hive QL

사용자가 커맨드 라인에서 MapReduce를 실행하거나, Pig Latin 스크립트를 실행하거나, Hive QL을 실행한다면 Resource Manager를 통해서 애플리케이션 ID를 배정받습니다.
Flamingo MapReduce Job Agent는 이 애플리케이션 ID를 별도로 기록하여 어떤 YARN 애플리케이션과 MapReduce Job이 어떤 워크플로우에 해당하는지를 알아냅니다.
물론 이 기능은 워크플로우 디자이너와 연계해서 동작하기 때문에 워크플로우 디자이너에서 수행한 MapReduce Job, Pig Latin, Hive QL의 경우에만 해당합니다.

MapReduce Job의 MapReduce Job ID 및 YARN 애플리케이션의 ID를 추출하기 위해서 다음와 같이 `HADOOP_CLIENT_OPTS` 환경변수에 Flamingo MapReduce Job Agent를 설정합니다.
이렇게 설정하면 MapReduce Job을 클라이언트에서 실행하는 경우 Job ID가 현재 디렉토리에 파일로 저장됩니다.

[source]
----
export HADOOP_CLIENT_OPTS=-javaagent:flamingo2-hadoop2-mr-agent-2.0.0.jar=resourcescript:mr2.bm
----

영구적으로 적용하고자 하는 경우 `~/.profile` 또는  `~/.bashrc` 파일에 설정할 수 있으며, `<HADOOP_HOME>/bin/hadoop` 커맨드에 환경변수를 설정할 수 있습니다.

Cloudera CDH를 사용하는 사용자의 경우도 동일하게 적용되며 다음의 커맨드를 통해서 동작 여부를 확인할 수 있습니다.

[source]
----
# hadoop jar hadoop-mapreduce-examples-2.5.0.jar wordcount /input.txt /output
15/02/15 13:18:02 INFO client.RMProxy: Connecting to ResourceManager at exo1.cdh.local/192.168.100.71:8032
15/02/15 13:18:09 INFO input.FileInputFormat: Total input paths to process : 1
15/02/15 13:18:09 INFO mapreduce.JobSubmitter: number of splits:1
15/02/15 13:18:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1423442407955_0150
15/02/15 13:18:10 INFO impl.YarnClientImpl: Submitted application application_1423442407955_0150
15/02/15 13:18:10 INFO mapreduce.Job: The url to track the job: http://exo1.cdh.local:8088/proxy/application_1423442407955_0150/
15/02/15 13:18:10 INFO mapreduce.Job: Running job: job_1423442407955_0150
15/02/15 13:19:09 INFO mapreduce.Job: Job job_1423442407955_0150 running in uber mode : false
15/02/15 13:19:09 INFO mapreduce.Job:  map 0% reduce 0%
15/02/15 13:20:30 INFO mapreduce.Job:  map 100% reduce 0%
15/02/15 13:20:42 INFO mapreduce.Job:  map 100% reduce 2%
15/02/15 13:20:43 INFO mapreduce.Job:  map 100% reduce 4%
15/02/15 13:20:44 INFO mapreduce.Job:  map 100% reduce 8%
15/02/15 13:20:45 INFO mapreduce.Job:  map 100% reduce 10%
15/02/15 13:20:48 INFO mapreduce.Job:  map 100% reduce 23%
15/02/15 13:20:49 INFO mapreduce.Job:  map 100% reduce 29%
15/02/15 13:20:50 INFO mapreduce.Job:  map 100% reduce 31%
15/02/15 13:20:51 INFO mapreduce.Job:  map 100% reduce 40%
15/02/15 13:20:52 INFO mapreduce.Job:  map 100% reduce 46%
15/02/15 13:20:53 INFO mapreduce.Job:  map 100% reduce 50%
15/02/15 13:20:54 INFO mapreduce.Job:  map 100% reduce 52%
15/02/15 13:20:58 INFO mapreduce.Job:  map 100% reduce 58%
15/02/15 13:20:59 INFO mapreduce.Job:  map 100% reduce 60%
15/02/15 13:21:00 INFO mapreduce.Job:  map 100% reduce 67%
15/02/15 13:21:01 INFO mapreduce.Job:  map 100% reduce 69%
15/02/15 13:21:02 INFO mapreduce.Job:  map 100% reduce 73%
15/02/15 13:21:03 INFO mapreduce.Job:  map 100% reduce 77%
15/02/15 13:21:04 INFO mapreduce.Job:  map 100% reduce 83%
15/02/15 13:21:05 INFO mapreduce.Job:  map 100% reduce 88%
15/02/15 13:21:06 INFO mapreduce.Job:  map 100% reduce 96%
15/02/15 13:21:14 INFO mapreduce.Job:  map 100% reduce 98%
15/02/15 13:21:17 INFO mapreduce.Job:  map 100% reduce 100%
15/02/15 13:21:18 INFO mapreduce.Job: Job job_1423442407955_0150 completed successfully
15/02/15 13:21:18 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=3179874
                FILE: Number of bytes written=11566880
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=4067777
                HDFS: Number of bytes written=4422649
                HDFS: Number of read operations=147
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=96
        Job Counters
                Launched map tasks=1
                Launched reduce tasks=48
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=77777
                Total time spent by all reduces in occupied slots (ms)=620248
                Total time spent by all map tasks (ms)=77777
                Total time spent by all reduce tasks (ms)=620248
                Total vcore-seconds taken by all map tasks=77777
                Total vcore-seconds taken by all reduce tasks=620248
                Total megabyte-seconds taken by all map tasks=79643648
                Total megabyte-seconds taken by all reduce tasks=635133952
        Map-Reduce Framework
                Map input records=354984
                Map output records=354984
                Map output bytes=5132627
                Map output materialized bytes=3179682
                Input split bytes=102
                Combine input records=354984
                Combine output records=354983
                Reduce input groups=354983
                Reduce shuffle bytes=3179682
                Reduce input records=354983
                Reduce output records=354983
                Spilled Records=709966
                Shuffled Maps =48
                Failed Shuffles=0
                Merged Map outputs=48
                GC time elapsed (ms)=40837
                CPU time spent (ms)=334420
                Physical memory (bytes) snapshot=14881824768
                Virtual memory (bytes) snapshot=69589762048
                Total committed heap usage (bytes)=38817759232
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=4067675
        File Output Format Counters
                Bytes Written=4422649
----

만약에 `HADOOP_CLIENT_OPTS` 옵션에 `-Dflamingo.debug=true` 을 같이 추가하면 다음과 같이 추가적인 로그를 확인할 수 있습니다. 보통 이 기능은 디버깅 하는 용도로만 사용하므로 일반적인 경우에는 사용하지 않아도 됩니다.

====
[source]
----
# hadoop jar hadoop-mapreduce-examples-2.5.0.jar wordcount /input.txt /output
15/02/15 13:18:02 INFO client.RMProxy: Connecting to ResourceManager at exo1.cdh.local/192.168.100.71:8032
15/02/15 13:18:09 INFO input.FileInputFormat: Total input paths to process : 1
15/02/15 13:18:09 INFO mapreduce.JobSubmitter: number of splits:1
15/02/15 13:18:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1423442407955_0150
************************************************************************************ # <1>
** YARN App instrumented By Flamingo 2 >> Path : /root
** YARN App instrumented By Flamingo 2 >> Application ID : application_1423442407955_0150
************************************************************************************
15/02/15 13:18:10 INFO impl.YarnClientImpl: Submitted application application_1423442407955_0150
15/02/15 13:18:10 INFO mapreduce.Job: The url to track the job: http://exo1.cdh.local:8088/proxy/application_1423442407955_0150/
************************************************************************************ # <2>
** MR instrumented By Flamingo 2 >> Path : /root
** MR instrumented By Flamingo 2 >> Job ID : job_1423442407955_0150
************************************************************************************
15/02/15 13:18:10 INFO mapreduce.Job: Running job: job_1423442407955_0150
15/02/15 13:19:09 INFO mapreduce.Job: Job job_1423442407955_0150 running in uber mode : false
15/02/15 13:19:09 INFO mapreduce.Job:  map 0% reduce 0%
15/02/15 13:20:30 INFO mapreduce.Job:  map 100% reduce 0%
15/02/15 13:20:42 INFO mapreduce.Job:  map 100% reduce 2%
15/02/15 13:20:43 INFO mapreduce.Job:  map 100% reduce 4%
15/02/15 13:20:44 INFO mapreduce.Job:  map 100% reduce 8%
15/02/15 13:20:45 INFO mapreduce.Job:  map 100% reduce 10%
15/02/15 13:20:48 INFO mapreduce.Job:  map 100% reduce 23%
15/02/15 13:20:49 INFO mapreduce.Job:  map 100% reduce 29%
15/02/15 13:20:50 INFO mapreduce.Job:  map 100% reduce 31%
15/02/15 13:20:51 INFO mapreduce.Job:  map 100% reduce 40%
15/02/15 13:20:52 INFO mapreduce.Job:  map 100% reduce 46%
15/02/15 13:20:53 INFO mapreduce.Job:  map 100% reduce 50%
15/02/15 13:20:54 INFO mapreduce.Job:  map 100% reduce 52%
15/02/15 13:20:58 INFO mapreduce.Job:  map 100% reduce 58%
15/02/15 13:20:59 INFO mapreduce.Job:  map 100% reduce 60%
15/02/15 13:21:00 INFO mapreduce.Job:  map 100% reduce 67%
15/02/15 13:21:01 INFO mapreduce.Job:  map 100% reduce 69%
15/02/15 13:21:02 INFO mapreduce.Job:  map 100% reduce 73%
15/02/15 13:21:03 INFO mapreduce.Job:  map 100% reduce 77%
15/02/15 13:21:04 INFO mapreduce.Job:  map 100% reduce 83%
15/02/15 13:21:05 INFO mapreduce.Job:  map 100% reduce 88%
15/02/15 13:21:06 INFO mapreduce.Job:  map 100% reduce 96%
15/02/15 13:21:14 INFO mapreduce.Job:  map 100% reduce 98%
15/02/15 13:21:17 INFO mapreduce.Job:  map 100% reduce 100%
15/02/15 13:21:18 INFO mapreduce.Job: Job job_1423442407955_0150 completed successfully
15/02/15 13:21:18 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=3179874
                FILE: Number of bytes written=11566880
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=4067777
                HDFS: Number of bytes written=4422649
                HDFS: Number of read operations=147
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=96
        Job Counters
                Launched map tasks=1
                Launched reduce tasks=48
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=77777
                Total time spent by all reduces in occupied slots (ms)=620248
                Total time spent by all map tasks (ms)=77777
                Total time spent by all reduce tasks (ms)=620248
                Total vcore-seconds taken by all map tasks=77777
                Total vcore-seconds taken by all reduce tasks=620248
                Total megabyte-seconds taken by all map tasks=79643648
                Total megabyte-seconds taken by all reduce tasks=635133952
        Map-Reduce Framework
                Map input records=354984
                Map output records=354984
                Map output bytes=5132627
                Map output materialized bytes=3179682
                Input split bytes=102
                Combine input records=354984
                Combine output records=354983
                Reduce input groups=354983
                Reduce shuffle bytes=3179682
                Reduce input records=354983
                Reduce output records=354983
                Spilled Records=709966
                Shuffled Maps =48
                Failed Shuffles=0
                Merged Map outputs=48
                GC time elapsed (ms)=40837
                CPU time spent (ms)=334420
                Physical memory (bytes) snapshot=14881824768
                Virtual memory (bytes) snapshot=69589762048
                Total committed heap usage (bytes)=38817759232
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=4067675
        File Output Format Counters
                Bytes Written=4422649
----
<1> YARN 애플리케이션 ID 추출
<2> MapReduce Job ID 추출
====

이렇게 Flamingo MapReduce Job Agent를 등록하면 아래와 같이 실행후 관련 정보를 파일로 기록합니다.
Flamingo 2의 모니터링 화면에서 Job Kill을 수행할때 사용하는 정보입니다.

[source,xml]
----
# cat app.application_1423442407955_0150
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
<comment/>
<entry key="queue">default</entry>
<entry key="applicationName">word count</entry>
<entry key="applicationId">application_1423442407955_0150</entry>
</properties>

# cat hadoop.job_1423442407955_0150
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
<comment/>
<entry key="jobId">job_1423442407955_0150</entry>
<entry key="queue">default</entry>
<entry key="trackingUrl">http://exo1.cdh.local:8088/proxy/application_1423442407955_0150/</entry>
<entry key="user">root</entry>
<entry key="jobName">word count</entry>
</properties>
----
